# 第五章：損失函數與優化方法

## 5.1 損失函數的作用與設計

### 什麼是損失函數？
損失函數（Loss Function）是機器學習中的核心概念，用於衡量模型的預測結果與真實標籤之間的差異。損失函數的大小直接反映了模型的預測效果，是模型訓練過程中需要最小化的目標。

---

### 為什麼損失函數重要？
1. **度量模型性能**：定量評估模型預測的準確性。
2. **指導參數更新**：模型的參數會根據損失函數的導數進行調整，從而逐步提高模型性能。
3. **反映業務目標**：選擇適合業務需求的損失函數能讓模型的預測結果更符合實際需求。

---

## 5.2 常見損失函數的介紹與應用場景

### 1. **回歸問題中的損失函數**

#### **1.1 均方誤差（MSE, Mean Squared Error）**
- **公式**：
  MSE = (1 / n) * Σ(y_i - ŷ_i)²
  - \(y_i\)：真實值。
  - \(ŷ_i\)：模型預測值。
  - \(n\)：樣本數。

- **特點**：
  1. 對大誤差更敏感，適合需要懲罰大偏差的應用場景。
  2. 適用於連續型目標（如房價預測）。

- **應用場景**：
  - 房價預測、產品銷量預測。

---

#### **1.2 平均絕對誤差（MAE, Mean Absolute Error）**
- **公式**：
  MAE = (1 / n) * Σ|y_i - ŷ_i|

- **特點**：
  1. 對異常值更穩定（不像 MSE 對大誤差懲罰那麼嚴重）。
  2. 更適合數據中存在異常值的情況。

- **應用場景**：
  - 預測用戶月均支出。

---

#### **1.3 Huber 損失（Huber Loss）**
- **公式**：
  Huber(a) = 0.5 * a²  (if |a| <= δ),  
             δ * (|a| - 0.5 * δ) (if |a| > δ)  
  - \(a = y_i - ŷ_i\)：預測值與真實值的差異。
  - \(\delta\)：控制異常值影響的閾值。

- **特點**：
  1. 結合了 MSE 和 MAE 的優點，對小誤差採用二次損失，對大誤差採用線性損失。
  2. 更加平衡異常值的影響。

- **應用場景**：
  - 當數據中既有異常值又要求高精度時（如金融數據分析）。

---

### 2. **分類問題中的損失函數**

#### **2.1 交叉熵損失（Cross-Entropy Loss）**
- **公式**：
  Cross-Entropy = - (1 / n) * Σ(y_i * log(ŷ_i))
  - \(y_i\)：真實類別的標籤（1 或 0）。
  - \(ŷ_i\)：模型對類別 1 的預測概率。

- **特點**：
  1. 適用於二分類和多分類問題。
  2. 對概率值的區別非常敏感，對於概率分佈的學習效果較好。

- **應用場景**：
  - 垃圾郵件分類、圖像分類。

---

#### **2.2 Hinge 損失**
- **公式**：
  Hinge Loss = max(0, 1 - y * ŷ)
  - \(y \in {-1, 1}\)：真實標籤。
  - \(ŷ\)：模型的預測結果。

- **特點**：
  1. 用於支持向量機（SVM）中，對錯誤分類的懲罰比交叉熵損失更平滑。
  2. 更注重分類邊界的優化。

---

### 3. **多目標問題中的損失函數**

#### **3.1 MultiMSE**
- **公式**：
  MultiMSE = (1 / m) * Σ (1 / n) * Σ(y_ij - ŷ_ij)²
  - \(m\)：目標數量。
  - \(n\)：樣本數量。

- **應用場景**：
  - 多輸入多輸出的回歸問題，如機器學習模型同時預測多個指標。

---

## 5.3 損失函數與優化方法的關聯

### 梯度與損失函數的關係
- **梯度下降法**：通過計算損失函數的導數（梯度），找到損失函數的最小值，從而更新模型參數。
- **公式**：
  θ = θ - η * ∇_θ L(θ)
  - θ：模型參數。
  - η：學習率。
  - L(θ)：損失函數。

---

## 5.4 梯度下降法及其變體

### 梯度下降的基本原理
- **目標**：通過損失函數的梯度，逐步接近最小值。
- **主要方法**：
  1. **批量梯度下降（Batch Gradient Descent）**：每次用全部數據計算梯度。
  2. **隨機梯度下降（SGD）**：每次用一個樣本計算梯度。
  3. **小批量梯度下降（Mini-Batch GD）**：每次用一部分樣本計算梯度。

---

### 梯度下降的常見變體
1. **Momentum**
   - 為梯度下降引入慣性項，能加速收斂。
   - 更新公式：
     v = γ * v - η * ∇_θ L(θ),  
     θ = θ + v

2. **Adam（Adaptive Moment Estimation）**
   - 自動調整學習率，適合非平穩目標。
   - 更新公式基於一階與二階動量的估計。

---

## 總結
本章詳細介紹了損失函數的重要性及常見類型，並深入分析了梯度下降法及其變體。損失函數是模型性能的衡量指標，而優化方法則是模型學習的基礎工具。理解這些概念將有助於靈活應對各種機器學習任務。
