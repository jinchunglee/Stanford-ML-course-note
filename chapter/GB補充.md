# 梯度提升與自動擬合的完整流程

## 數據集與基本設置

假設數據集如下：

| \(x\)  | \(y\)  |
|--------|--------|
| 1.0    | 10     |
| 2.0    | 20     |
| 3.0    | 30     |

- **損失函數**：均方誤差（MSE）。
- **學習率**：\(\eta = 0.1\)。

目標：通過梯度提升框架構建模型，逐步縮小預測值 \(\hat{y}\) 與真實值 \(y\) 之間的誤差，並解釋 **Weighting Increase** 是如何實現的。

---

## 第一輪迭代

### 步驟 1：初始化模型

初始預測值為目標值的均值，這是模型的初始輸出：
\[
\hat{y} = \frac{1}{n} \sum_{i=1}^n y_i = \frac{10 + 20 + 30}{3} = 20
\]

| \(x\)  | 初始預測值 \(\hat{y}_i\) |
|--------|--------------------------|
| 1.0    | 20                       |
| 2.0    | 20                       |
| 3.0    | 20                       |

---

### 步驟 2：計算殘差

殘差 \(r_i\) 表示真實值 \(y_i\) 與當前預測值 \(\hat{y}_i\) 之間的誤差：
\[
r_i = y_i - \hat{y}_i
\]

計算得到的殘差如下：

| \(x\)  | \(y\)  | \(\hat{y}_i\) | 殘差 \(r_i = y_i - \hat{y}_i\) |
|--------|--------|---------------|--------------------------------|
| 1.0    | 10     | 20            | \(10 - 20 = -10\)             |
| 2.0    | 20     | 20            | \(20 - 20 = 0\)               |
| 3.0    | 30     | 20            | \(30 - 20 = 10\)              |

這些殘差成為第一棵決策樹的擬合目標。

---

### 步驟 3：擬合第一棵決策樹

#### 3.1 生成分裂點候選

對數值型特徵 \(x\)，生成分裂點候選：
- 將特徵值按升序排序：\([1.0, 2.0, 3.0]\)。
- 取相鄰值的中點作為候選分裂點：
  \[
  t \in \{1.5, 2.5\}
  \]

---

#### 3.2 計算每個分裂點的損失

對每個候選分裂點 \(t\)，計算分裂後的損失 \(L\)，公式為：
\[
L = \sum_{\text{葉節點}} \frac{1}{n} \sum_{i \in \text{葉節點}} (r_i)^2
\]

- **分裂點 \(t = 1.5\)**：
  - 左子樹：\(x \leq 1.5\)，包含樣本 \([1.0]\)，殘差為 \([-10]\)。
  - 右子樹：\(x > 1.5\)，包含樣本 \([2.0, 3.0]\)，殘差為 \([0, 10]\)。
  - 損失計算：
    \[
    L = \frac{1}{1}(-10)^2 + \frac{1}{2}(0^2 + 10^2) = 100 + 50 = 150
    \]

- **分裂點 \(t = 2.5\)**：
  - 左子樹：\(x \leq 2.5\)，包含樣本 \([1.0, 2.0]\)，殘差為 \([-10, 0]\)。
  - 右子樹：\(x > 2.5\)，包含樣本 \([3.0]\)，殘差為 \([10]\)。
  - 損失計算：
    \[
    L = \frac{1}{2}((-10)^2 + 0^2) + \frac{1}{1}(10^2) = 50 + 100 = 150
    \]

選擇損失最小的分裂點，假設為 \(t = 1.5\)。

---

### 步驟 4：計算葉節點的輸出值

#### 4.1 葉節點輸出值公式的推導

葉節點的輸出值 \(f(x)\) 是通過最小化葉節點內的損失函數來計算的。公式推導如下：

1. **定義葉節點的損失函數**：
   \[
   L = \sum_{i=1}^{n} \left( y_i - (\hat{y}_i + f) \right)^2
   \]

2. **展開損失函數**：
   \[
   L = \sum_{i=1}^n \left( y_i - \hat{y}_i - f \right)^2
   \]
   展開為：
   \[
   L = \sum_{i=1}^n \left[ \left( y_i - \hat{y}_i \right)^2 - 2 \cdot \left( y_i - \hat{y}_i \right) \cdot f + f^2 \right]
   \]

3. **整理**：
   \[
   L = \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 - 2f \sum_{i=1}^n \left( y_i - \hat{y}_i \right) + n f^2
   \]

4. **對 \(f\) 求導並令其為零**：
   \[
   \frac{\partial L}{\partial f} = -2 \sum_{i=1}^n \left( y_i - \hat{y}_i \right) + 2n f = 0
   \]
   解得：
   \[
   f = \frac{\sum_{i=1}^n r_i}{n}
   \]

---

#### 4.2 計算具體的葉節點輸出值

- **左子樹（\(x \leq 1.5\)）**：
  - 包含樣本 \([1.0]\)，殘差為 \([-10]\)。
  - 輸出值：
    \[
    f(x)_{\text{左}} = \frac{-10}{1} = -10
    \]

- **右子樹（\(x > 1.5\)）**：
  - 包含樣本 \([2.0, 3.0]\)，殘差為 \([0, 10]\)。
  - 輸出值：
    \[
    f(x)_{\text{右}} = \frac{0 + 10}{2} = 5
    \]

---

### 步驟 5：更新預測值

使用葉節點輸出值更新預測值：
\[
\hat{y}_{\text{new}} = \hat{y}_{\text{old}} + \eta \cdot f(x)
\]

| \(x\)  | 初始預測值 \(\hat{y}_i\) | 葉節點輸出 \(f(x)\) | 更新後預測值 \(\hat{y}_{\text{new}}\) |
|--------|--------------------------|---------------------|---------------------------------------|
| 1.0    | 20                       | -10                | \(20 + 0.1 \cdot (-10) = 19\)        |
| 2.0    | 20                       | 5                  | \(20 + 0.1 \cdot 5 = 20.5\)          |
| 3.0    | 20                       | 5                  | \(20 + 0.1 \cdot 5 = 20.5\)          |

---

### 步驟 6：Weighting Increase 的作用

1. **誤差大的樣本影響力更大**：
   - 在下一棵樹中，殘差大的樣本（如 \(x=1.0\)）因梯度值（即殘差值）的絕對值更大，會對分裂規則和葉節點輸出有更大影響。

2. **隱式加權**：
   - Weighting Increase 並非顯式更改樣本權重，而是通過梯度的絕對值放大誤差大的樣本在擬合過程中的影響。

---

這樣可以直接複製到 Markdown 編輯器中顯示。如果還需要調整，請告訴我！ 😊
