# 第六章：決策樹模型

## 6.1 決策樹的概念

### 什麼是決策樹？
決策樹（Decision Tree）是一種基於樹形結構的機器學習模型，通過一系列的決策規則對數據進行分類或回歸。決策樹以易於理解和解釋的層次化結構建模，是許多機器學習算法（如隨機森林、梯度提升樹）的基礎。

---

### 決策樹的特點
1. **易於解釋**：
   - 決策樹的結構可以轉化為規則，類似人類的思維方式。
2. **適用於分類和回歸**：
   - 決策樹可以根據目標值的不同，分為分類樹（CART for Classification）和回歸樹（CART for Regression）。
3. **非線性建模能力**：
   - 不依賴數據的線性假設，適合處理複雜的非線性問題。
4. **對數據預處理要求低**：
   - 無需標準化或正規化，對缺失值和類別型特徵不敏感。

---

## 6.2 決策樹的結構

### 節點的類型
1. **根節點**：
   - 樹的起始節點，包含整個數據集。
2. **內部節點**：
   - 根據某個特徵進行劃分，生成子節點。
3. **葉節點**：
   - 樹的最終節點，包含一個類別（分類問題）或一個數值（回歸問題）。

### 節點的劃分方式
- 決策樹通過在每個節點選擇一個最佳劃分特徵，將數據進行分裂，直至達到停止條件。

---

## 6.3 決策樹的構建過程

### 1. 選擇最佳特徵
- 每次分裂時，選擇一個特徵作為分裂標準，使得目標函數（如基尼係數或信息增益）最優。

---

### 2. 劃分數據
- 根據所選特徵的值，將數據集分成多個子集。

---

### 3. 遞歸構建子樹
- 對每個子集重複以上步驟，直到滿足停止條件（如樹的深度或葉節點數量）。

---

### 4. 停止條件
- 樹的構建過程會在以下情況下停止：
  1. 樹達到預設的最大深度。
  2. 每個葉節點中的樣本數量小於最小樣本數。
  3. 節點中所有樣本屬於同一類。

---

## 6.4 核心算法

### 1. ID3（Iterative Dichotomiser 3）
- **目標**：最大化信息增益。
- **公式**：
  $$
  \text{Information Gain (IG)} = H(D) - \sum_{i=1}^k \frac{|D_i|}{|D|} H(D_i)
  $$
  - \(H(D)\)：數據集的熵。
  - \(D_i\)：第 \(i\) 個子集。
- **特點**：
  - 熵計算複雜，對多分類問題效果較好。

---

### 2. C4.5
- **改進**：在 ID3 的基礎上引入信息增益率來解決特徵偏向問題。
- **公式**：
  $$
  \text{Gain Ratio} = \frac{\text{Information Gain}}{\text{Split Information}}
  $$
  - \( \text{Split Information} \)：分裂信息。

---

### 3. CART（Classification and Regression Trees）
- **分類樹**：使用基尼不純度（Gini Impurity）作為劃分標準。
  $$
  Gini(D) = 1 - \sum_{k=1}^K p_k^2
  $$
  - \(p_k\)：第 \(k\) 類的樣本比例。
- **回歸樹**：使用均方誤差（MSE）作為劃分標準。
  $$
  MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  $$
- **特點**：
  - 能處理分類與回歸問題，是目前最常用的決策樹算法。

---

## 6.5 決策樹的優化

### 1. 剪枝技術
- **預剪枝（Pre-pruning）**：
  - 在樹構建過程中提前停止分裂。
  - **優點**：防止過擬合。
  - **缺點**：可能導致欠擬合。

- **後剪枝（Post-pruning）**：
  - 先構建完全的決策樹，再通過評估節點刪除不必要的子樹。
  - **優點**：更穩健。
  - **缺點**：計算成本高。

---

### 2. 控制樹的深度
- 限制樹的最大深度可以有效防止過擬合。

---

### 3. 引入隨機性
- 在特徵選擇中引入隨機性（如隨機森林中的隨機特徵子集選擇）可以提升泛化能力。

---

## 6.6 決策樹的優勢與局限

### 優勢
1. **易於解釋**：樹的結構可以清晰地表示決策過程。
2. **低數據預處理需求**：對類別型特徵和缺失值不敏感。
3. **靈活性強**：可處理非線性關係。

---

### 局限
1. **容易過擬合**：未剪枝的樹通常表現不穩定。
2. **對噪聲敏感**：少量異常數據可能極大影響樹的結構。
3. **不穩定性**：對數據的微小變化可能導致樹的結構劇烈改變。

---

## 6.7 決策樹的應用場景

1. **分類問題**：
   - 客戶流失預測。
   - 癌症診斷（基於基因數據）。
2. **回歸問題**：
   - 房價預測。
   - 能源消耗分析。

---

## 6.8 **自動擬合規則專區**

### 決策樹如何進行自動擬合規則？
在決策樹中，自動擬合規則分為以下三個核心步驟：
1. **找到最佳分裂點 \(t\)**：確定如何劃分數據。
2. **計算葉節點的輸出值 \(f(x)\)**：根據葉節點內樣本目標值（如負梯度）計算輸出值。
3. **更新預測值**：將葉節點的輸出值加入當前預測結果，進一步修正。

以下通過具體公式和例子，詳細講解每一步驟。

---

### **步驟 1：找到最佳分裂點 \(t\)**

#### 核心原理
1. 枚舉所有可能的分裂點。
2. 根據分裂後的損失函數值選擇最優分裂點。

---

#### **數值型特徵的分裂點計算**
- 將特徵值排序，取相鄰值的中點作為候選分裂點。
  - 例如，特徵值 \([1.0, 2.0, 3.0]\)，分裂點候選為 \(1.5, 2.5\)。

---

#### **損失函數計算**
- 使用負梯度均方誤差（MSE）作為損失函數，計算分裂前後的總損失：
  \[
  L = \sum_{\text{葉節點}} \frac{1}{n} \sum_{i \in \text{葉節點}} (g_i)^2
  \]
  - \(g_i\)：樣本 \(i\) 的負梯度或殘差。
  - \(n\)：葉節點內的樣本數。

---

#### **具體例子**
假設數據集如下：

| \(x\) | \(y\) | 負梯度 \(g_i = y - \hat{y}\) |
|-------|-------|------------------------------|
| 1.0   | 10    | -2                           |
| 2.0   | 20    | 0                            |
| 3.0   | 30    | 2                            |

##### 嘗試分裂點 \(t = 1.5\)：
1. 左子樹：\(x \leq 1.5\)，樣本 \([1.0]\)，對應梯度 \([-2]\)。
2. 右子樹：\(x > 1.5\)，樣本 \([2.0, 3.0]\)，對應梯度 \([0, 2]\)。
3. 損失計算：
   \[
   L = \frac{1}{1}(-2)^2 + \frac{1}{2}(0^2 + 2^2) = 4 + 2 = 6
   \]

##### 嘗試分裂點 \(t = 2.5\)：
1. 左子樹：\(x \leq 2.5\)，樣本 \([1.0, 2.0]\)，對應梯度 \([-2, 0]\)。
2. 右子樹：\(x > 2.5\)，樣本 \([3.0]\)，對應梯度 \([2]\)。
3. 損失計算：
   \[
   L = \frac{1}{2}((-2)^2 + 0^2) + \frac{1}{1}(2^2) = 2 + 4 = 6
   \]

#### **選擇最優分裂點**
- 如果 \(t = 1.5\) 和 \(t = 2.5\) 的損失相同，可以任意選擇一個作為分裂點。
- 假設我們選擇 \(t = 1.5\)。

---

### **步驟 2：計算葉節點的輸出值 \(f(x)\)**

#### 核心原理
分裂後，每個葉節點的輸出值基於節點內樣本的目標值（如負梯度）計算。

---

#### **輸出值的典型計算方式**
1. **回歸問題**：
   - 用葉節點內樣本目標值的均值作為輸出值。
   - 公式：
     \[
     f(x) = \frac{1}{N} \sum_{i \in \text{葉節點}} g_i
     \]
     - \(g_i\)：樣本的目標值（如負梯度或殘差）。
     - \(N\)：葉節點內的樣本數。

2. **梯度提升框架**：
   - 葉節點擬合負梯度的均值。
   - 公式：
     \[
     f(x) = \frac{\sum_{i \in \text{葉節點}} g_i}{\text{樣本數}}
     \]

---

#### **具體例子**
基於上一例的分裂點 \(t = 1.5\)：

##### 左子樹葉節點（\(x \leq 1.5\)）：
1. 該葉節點內樣本梯度為 \([-2]\)。
2. 輸出值為：
   \[
   f(x)_{\text{左}} = \frac{-2}{1} = -2
   \]

##### 右子樹葉節點（\(x > 1.5\)）：
1. 該葉節點內樣本梯度為 \([0, 2]\)。
2. 輸出值為：
   \[
   f(x)_{\text{右}} = \frac{0 + 2}{2} = 1
   \]

---

### **步驟 3：更新預測值**

#### 更新公式
基於學習率 \(\eta\)，將葉節點輸出值加入到當前預測值中：
\[
\hat{y}_{\text{new}} = \hat{y}_{\text{old}} + \eta \cdot f(x)
\]

---

#### **具體例子**
假設初始預測值為 20，學習率 \(\eta = 0.1\)：

| \(x\) | 當前預測值 \(\hat{y}_{\text{old}}\) | 葉節點輸出 \(f(x)\) | 更新後預測值 \(\hat{y}_{\text{new}}\) |
|-------|-------------------------------------|----------------------|-------------------------------------|
| 1.0   | 20                                  | -2                   | \(20 + 0.1 \cdot (-2) = 19.8\)     |
| 2.0   | 20                                  | 1                    | \(20 + 0.1 \cdot 1 = 20.1\)        |
| 3.0   | 20                                  | 1                    | \(20 + 0.1 \cdot 1 = 20.1\)        |

---




## 6.9 相關問題專區

### 1. 決策樹如何處理類別型特徵？
- 類別型特徵可以直接用於劃分數據，不需要額外的數值轉換。

---

### 2. 為什麼基尼不純度比熵計算更快？
- 基尼不純度的公式更簡單，無需計算對數，因此計算效率更高。

---

### 3. 決策樹的基模型如何用於梯度提升？
- 在梯度提升中，每棵樹的輸出是基於當前模型的殘差，並用來修正預測值。

---

### 4. 剪枝的作用是什麼？
- 剪枝的目的是防止過擬合，提升模型在測試集上的泛化能力。

---

## 總結
本章全面介紹了決策樹的基本概念、構建過程、核心算法以及優化技術，並回答了與決策樹相關的常見問題。決策樹作為基礎模型，已廣泛應用於分類和回歸問題，是理解集成學習（如隨機森林和梯度提升）的基石。
