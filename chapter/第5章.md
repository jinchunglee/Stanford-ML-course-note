# 第五章：損失函數與優化方法

## 5.1 損失函數

### 什麼是損失函數？
損失函數（Loss Function）是衡量模型預測結果與真實標籤之間差距的函數，是機器學習模型訓練的核心部分。通過最小化損失函數，模型得以學習更準確的預測。

---

### 常見損失函數

#### 1. **均方誤差（MSE, Mean Squared Error）**
- **公式**：
  $$
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  $$
  - \(y_i\)：真實值。
  - \(\hat{y}_i\)：模型預測值。
  - \(n\)：樣本數。

- **適用場景**：
  - 用於回歸任務，尤其當大誤差需要被放大時。

- **例子**：
  - 房價預測中，若預測房價 \( \hat{y}_i = 200,000 \) 而實際房價 \( y_i = 210,000 \)，MSE 衡量預測值與實際值的平方誤差。

---

#### 2. **交叉熵損失（Cross-Entropy Loss）**
- **公式**：
  $$
  \text{Cross-Entropy} = - \frac{1}{n} \sum_{i=1}^n \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
  $$

- **適用場景**：
  - 二分類任務（如垃圾郵件分類）。
  - 多分類問題時需用擴展版本（Softmax Cross-Entropy）。

- **例子**：
  - 在垃圾郵件分類中，當預測 \( \hat{y}_i = 0.8 \)（為垃圾郵件的概率），而實際 \( y_i = 1 \)（真實為垃圾郵件），交叉熵衡量預測概率與真實標籤的一致性。

---

#### 3. **Huber 損失（Huber Loss）**
- **公式**：
  $$
  L_{\delta}(a) =
  \begin{cases}
    \frac{1}{2}a^2, & \text{if } |a| \leq \delta \\
    \delta \cdot (|a| - \frac{1}{2}\delta), & \text{if } |a| > \delta
  \end{cases}
  $$
  - \(a = y_i - \hat{y}_i\)。

- **適用場景**：
  - 回歸問題中結合均方誤差與平均絕對誤差的優點，對異常值更為穩健。

- **例子**：
  - 預測某設備的能耗，當某些異常數據（如極端天氣下的能耗峰值）存在時，Huber 損失能減少異常值對模型的影響。

---

### 如何選擇損失函數？
- 回歸問題：MSE、Huber Loss、MAE（平均絕對誤差）。
- 分類問題：交叉熵損失、Hinge 損失（支持向量機）。
- 多目標問題：MultiMSE 或自定義損失函數。

---

## 5.2 優化方法

### 什麼是優化方法？
優化方法（Optimization Method）是用於最小化損失函數的算法，通過調整模型參數，使模型預測結果更接近真實值。

---

### 常見優化方法

#### 1. **梯度下降法（Gradient Descent）**
- **原理**：
  - 根據損失函數的導數（梯度）來更新模型參數，沿損失函數下降最快的方向移動。

- **公式**：
  $$
  \theta = \theta - \eta \cdot \nabla_\theta L(\theta)
  $$
  - \(\theta\)：模型參數。
  - \(\eta\)：學習率。
  - \(L(\theta)\)：損失函數。

- **變體**：
  1. **批量梯度下降（Batch Gradient Descent）**：
     - 使用全量數據計算梯度，更新參數。
  2. **隨機梯度下降（SGD, Stochastic Gradient Descent）**：
     - 使用單一樣本計算梯度，更新參數。
  3. **小批量梯度下降（Mini-Batch Gradient Descent）**：
     - 使用一小部分數據計算梯度，結合批量和隨機的優點。

- **例子**：
  - 在線性回歸中，根據均方誤差的梯度更新權重，逐步逼近最佳參數。

---

#### 2. **動量法（Momentum）**
- **原理**：
  - 為梯度更新引入動量項，加速收斂並減少震盪。

- **公式**：
  $$
  v_t = \gamma v_{t-1} + \eta \nabla_\theta L(\theta)
  $$
  $$
  \theta = \theta - v_t
  $$
  - \(\gamma\)：動量係數（如 0.9）。

- **優勢**：
  - 加速收斂，特別在非凸函數的優化中。

---

#### 3. **Adam（Adaptive Moment Estimation）**
- **原理**：
  - 結合動量法與自適應學習率調整技術，對每個參數分別設置學習率。

- **公式**：
  1. 計算梯度的一階動量和二階動量：
     $$
     m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta L(\theta)
     $$
     $$
     v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_\theta L(\theta))^2
     $$
  2. 更新參數：
     $$
     \theta = \theta - \eta \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}
     $$

- **優勢**：
  - 適用於稀疏數據和大型模型，收斂速度快。
  - 是深度學習中最常用的優化算法之一。

- **例子**：
  - 用於訓練 CNN 或 RNN 時處理高維數據。

---

### 如何選擇優化方法？
1. **簡單模型（如線性回歸）**：批量梯度下降或 SGD。
2. **深度學習模型（如 CNN、RNN）**：Adam、RMSprop。
3. **快速收斂需求**：動量法、Nesterov 加速梯度。

---

## 總結
- **損失函數** 是模型學習的核心，用於衡量預測與實際值的差距。
- **優化方法** 則是模型參數調整的手段，通過不斷最小化損失函數，使模型表現更優。
- 本章通過詳細的理論解釋與實際例子，幫助讀者掌握如何選擇損失函數與優化方法，應用於實際機器學習任務中。
