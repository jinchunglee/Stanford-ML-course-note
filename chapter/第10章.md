# 第十章：模型性能評估

## 10.1 模型評估的重要性

### 為什麼需要模型評估？
在機器學習中，模型的性能直接影響預測結果的可靠性與實際應用效果。模型評估的主要目的包括：
1. 衡量模型的預測能力。
2. 幫助選擇最佳模型或參數。
3. 發現模型的潛在問題（如過擬合或欠擬合）。
4. 提高模型的泛化能力。

---

## 10.2 評估指標

### 1. 回歸問題的評估指標
#### **1.1 均方誤差（MSE, Mean Squared Error）**
- 公式：
  \[
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  \]
  - \(y_i\)：真實值。
  - \(\hat{y}_i\)：預測值。
  - \(n\)：樣本數。
- **解釋**：MSE 衡量預測值與真實值之間的平方誤差，對於大誤差更為敏感。

#### **1.2 平均絕對誤差（MAE, Mean Absolute Error）**
- 公式：
  \[
  \text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
  \]
- **解釋**：MAE 測量預測值與真實值之間的絕對誤差，對異常值更穩健。

#### **1.3 決定係數（R², Coefficient of Determination）**
- 公式：
  \[
  R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
  \]
  - \( \bar{y} \)：目標值的均值。
- **解釋**：R² 衡量模型解釋數據變異的能力，值越接近 1，模型效果越好。

---

### 2. 分類問題的評估指標
#### **2.1 準確率（Accuracy）**
- 公式：
  \[
  \text{Accuracy} = \frac{\text{正確分類的樣本數}}{\text{總樣本數}}
  \]
- **解釋**：衡量模型正確分類的比例，適合類別分佈均衡的數據集。

#### **2.2 精確率與召回率（Precision and Recall）**
- **精確率**（Precision）：
  \[
  \text{Precision} = \frac{\text{TP}}{\text{TP + FP}}
  \]
- **召回率**（Recall）：
  \[
  \text{Recall} = \frac{\text{TP}}{\text{TP + FN}}
  \]
  - TP：真正類數量。
  - FP：假正類數量。
  - FN：假負類數量。

#### **2.3 F1-Score**
- 公式：
  \[
  F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
  \]
- **解釋**：F1-Score 是 Precision 和 Recall 的調和平均，適合類別不均衡的數據集。

#### **2.4 ROC-AUC（Receiver Operating Characteristic - Area Under Curve）**
- **解釋**：ROC 曲線展示模型在不同分類閾值下的性能，AUC（曲線下的面積）越接近 1，模型越優秀。

---

## 10.3 評估方法

### 1. 訓練集和測試集的劃分
- 將數據分為訓練集（Training Set）和測試集（Test Set）。
- 常見比例：80% 訓練集 + 20% 測試集。

---

### 2. 交叉驗證（Cross-Validation）
- 將數據分成 \(k\) 個子集，依次將其中一個子集作為測試集，其餘子集作為訓練集。
- **優勢**：
  1. 更穩健的模型評估。
  2. 適合數據量較小的情況。

---

### 3. 留一法驗證（Leave-One-Out Cross-Validation, LOOCV）
- 將數據集中的每個樣本依次作為測試集，其餘樣本作為訓練集。
- **優勢**：充分利用數據。
- **劣勢**：計算成本高。

---

## 10.4 過擬合與欠擬合的診斷與解決

### 1. 過擬合（Overfitting）
- **表現**：模型在訓練集上表現良好，但在測試集上效果不佳。
- **原因**：
  1. 模型過於複雜。
  2. 樣本數據不足。
- **解決方法**：
  1. 使用正則化（如 L1 或 L2）。
  2. 減少模型複雜度（如降低決策樹深度）。
  3. 增加訓練數據。

---

### 2. 欠擬合（Underfitting）
- **表現**：模型無法很好地擬合訓練數據，表現較差。
- **原因**：
  1. 模型過於簡單。
  2. 特徵不足或不夠準確。
- **解決方法**：
  1. 增加模型的複雜度。
  2. 提高特徵工程質量。

---

## 10.5 模型選擇的實際案例

### 案例 1：回歸問題中的房價預測
1. 使用 MSE 和 R² 評估模型性能。
2. 交叉驗證確保模型的穩健性。
3. 在發現過擬合後，降低模型複雜度（如限制決策樹深度）。

---

### 案例 2：分類問題中的垃圾郵件分類
1. 使用 Precision、Recall 和 F1-Score 評估模型。
2. 如果類別不均衡，優化模型的閾值以提升 Recall。
3. 通過 ROC-AUC 確保模型在不同閾值下的穩定性。

---

## 總結
本章重點介紹了模型性能評估的指標（如 MSE、F1-Score、ROC-AUC）和方法（如交叉驗證）。通過正確的模型評估，不僅能提升模型的預測性能，還能有效避免過擬合與欠擬合。下一章將討論深度學習模型的架構與應用。
